{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### <img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
     "\n",
     "## Inteligencia Artificial - IAI84\n",
     "## Trabajo Final\n",
     "### Instituto Tecnológico Metropolitano"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "<hr>\n",
     "- __Porcentaje de evaluación__: 20%  5% de recuperación\n",
     "- __Entregable__: Notebook de jupyter o scripts en Python con material informativo y códigos fuente.\n",
     "- __Forma de entrega__: Sustentación en clase  correo electrónico a pedroatencio@itm.edu.co - asunto: IA_Taller4_nombre1_nombre2_nombre3_nombre4\n",
     "- __Fecha de asignación__: Martes 14 de noviembre de 2017\n",
     "- __Fecha de entrega__: Sábado 25 de noviembre de 2017."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "<hr>\n",
     "## <font color=\"red\">Importante!</font>\n",
     "Todos los grupos deben DOCUMENTAR el proceso de analisis, diseño y construcción de sus soluciones, así como los experimentos que fueron realizados para validar que efectivamente, los toolboxes funcionan correctamente (etapas 1, 2 y 3). En el caso de la etapa 4, la experimentación planteada en los requerimientos, su análisis y resultados constituyen la documentación. El objetivo de esta documentación es tener material de discusión el día de la sustentación.\n",
     "\n",
     "## <font color=\"red\">Importante!</font>\n",
     "El trabajo tendrá un componente de calificación individual por grupo (70%) y un componente de calificación global (30%) que medirá la capacidad de integración de los distintos equipos y el resultado final."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "<hr>\n",
     "## Introducción\n",
     "El objetivo general de este trabajo consiste en utilizar los conceptos de redes neuronales y redes neuronales profundas revisados en clase, para implementar un clasificador de imágenes. Este clasificador debe diferenciar o reconocer si una imagen de entrada contiene un perro o un gato. Para ello será utilizada una base de datos ampliamente conocida por la comunidad de inteligencia artifial: CatsvsDogs. \n",
     "\n",
     "El trabajo está diseñado para ser realizado por 4 equipos de trabajo, cada uno responsable de una etapa del proyecto. Cada equipo debe suministrar un resultado adecuado al equipo encargado de la etapa siguiente y a su vez, los equipos deberán autoregularse exigiendo a los equipos de etapas anteriores, un resultado adecuado.\n",
     "\n",
     "Las etapas del proyecto se pueden observar en la siguiente figura:\n",
     "<br><br>\n",
     "<center><img src=\"res/etapas_2.png\" width=800></center>\n",
     "\n",
     "A continuación se especifican los requerimientos de cada etapa."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {
     "collapsed": true
    },
    "source": [
     "<hr>\n",
     "## Etapa 1: Preparación de datos\n",
     "\n",
     "### __Requerimiento general__\n",
     "Diseñar y desarrollar un toolbox (funciones / clases) y documentación técnica del mismo, para leer el dataset de imágenes (Dogs vs Cats), entregar los datos en formato vectorial, consultar imágenes por índice y generar los datasets de entrenamiento, prueba y validación, entre otras tareas.\n",
     "\n",
     "### __Dataset__\n",
     "El dataset de trabajo será Dogs vs Cats, el cuál puede ser descargado en el siguiente link: https://www.kaggle.com/c/dogs-vs-cats/data.\n",
     "\n",
     "El dataset completo contiene 25000 imágenes de perros y gatos. En el link anterior, se pueden descargar dos archivos.zip: __train.zip__ y __test1.zip__. Estos archivos se contienen imágenes para entrenamiento e imágenes para pruebas respectivamente. <font color=\"red\">Debido a la cantidad de imágenes en el dataset completo, solo es necesario descargar el archivo __train.zip__</font>.\n",
     "\n",
     "El dataset contiene dos carpetas principales, __dogs__ y __cats__ que contienen imágenes de perros y gatos respectivamente. Las imágenes están separadas con el objetivo de conocer la clase real de la imagen que se desea procesar.\n",
     "\n",
     "### __Requerimientos específicos__ (mínimo)\n",
     "El toolbox creado debe cumplir con los siguientes requerimientos:\n",
     "- Debe permitir la lectura de imágenes de la carpeta del dataset Dogs vs Cats. Esta lectura debe realizarse de forma aleatoria con el objetivo de reducir cualquier distribución en las imágenes del dataset.\n",
     "- Debe cambiar el alto y ancho de las imágenes a $(64,64)$. El número de canales se debe mantener en 3 (R,G y B).\n",
     "- Debe permitir obtener la representación vectorial de las imágenes del dataset. Por ejemplo, cada imagen de tamaño: $(64,64,3)$ píxeles, debe transformase en un vector de forma: $(12288,1)$, donde $12288=64\\times64\\times3$.\n",
     "- Debe permitir la generación de los datasets de: entrenamiento, prueba y validación (TRAIN/DEV/TEST). Esta función debe recibir como argumento el número de imágenes que se desea leer, por ejemplo, 1000, y retornar las matrices de características **X_TRAIN**, **X_DEV**, **X_TEST** y los vectores de categorías **Y_TRAIN**, **Y_DEV**, **Y_TEST**, asociados a las matrices de características anteriores. Los datasets deben tener la siguiente proporción: \n",
     "    - TRAIN (60%)\n",
     "    - DEV (20%)\n",
     "    - TEST (20%)\n",
     "- Los tamaños de las matrices $X$'s deben ser $(12288, m)$, donde $m$ se refiere al número de ejemplos.\n",
     "- Los tamaños de los vectores $Y$´s deben ser $(1, m)$, donde $m$ se refiere al número de ejemplos.\n",
     "- Los datasets generados, deben tener igual proporción de imágenes de perros y de gatos.\n",
     "- El toolbox debe permitir __reversar__ una imagen en forma vectorial de tamaño $(12288, 1)$ a su forma matricial $(64,64,3)$.\n",
     "- El toolbox debe permitir __visualizar__ (pyplot) una imagen y su categoría, dado el índice dentro del dataset.\n",
     "- El toolbox debe permitir __normalizar__ los datos. Esto puede implementarse utilizando un argumento extra en la función de generación del dataset, que permita decidir si se quieren generar los datos normalizados o no."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "<hr>\n",
     "## Etapa 2: Diseño de la arquitectura de red neuronal \n",
     "\n",
     "### __Requerimiento general__\n",
     "Diseñar y desarrollar un toolbox (funciones / clases) y documentación técnica del mismo, que permita construir una red neuronal general en términos del número de capas, cantidad de neuronas por capas y funciones de activación de cada capa, así como de realizar propagación hacia adelante de la red construida. La capa de entrada $L^{[0]}$ de esta red siempre será un vector de $(12288, 1)$ y la salida siempre será de $(1,1)$. La última capa de la red siempre tendrá una función de activación sigmoide, debido a que el problema que se pretende resolver es un clasificación binaria.\n",
     "\n",
     "### __Requerimientos específicos__ (mínimo)\n",
     "El toolbox creado debe cumplir con los siguientes requerimientos:\n",
     "- Debe permitir generar un red neuronal artificial, ingresando como __argumentos__, el número de capas ocultas, la cantidad de neuronas por cada capa oculta y la función de activación de cada capa oculta. La red siempre tendrá como capa de entrada un vector de tamaño $(12288, 1)$ y la capa de salida será de tamaño $(1,1)$ con función de activación sigmoide $\\sigma(z)$.\n",
     "- Las capas ocultas pueden tener funciones de activacion sigmoide $\\sigma(z)$ o tangente hiperbólica $tanh(z)$. El usuario puede especificar el tipo de función de activación de las capas ocultas de forma individual.\n",
     "- La cantidad de neuronas de cada capa oculta puede ser diferente, por ejemplo, una red podría tener dos capas ocultas, la primera con 10 neuronas y la segunda con 3 neuronas.\n",
     "- Debe inicializar de forma automática, los pesos utilizando __Xavier's Initialization__ y bias en ceros.\n",
     "- Debe utilizar el toolbox de la etapa 1 para acceder a los datos.\n",
     "- Debe poder calcular la propagación hacia adelante (forward propagation) dado un vector (1 ejemplo) o una matriz ($m$ ejemplos) de entrada.\n",
     "- Debe permitir obtener a los resultados parciales $z$ y $a$ de cada capa, una vez realizado el proceso de propagación hacia adelante.\n",
     "- Debe permitir obtener los parámetros $W$ y $b$ de cada capa en cualquier momento.\n",
     "- La red generada debe poder ser manipulada como una estructura de datos o un objeto."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {
     "collapsed": true
    },
    "source": [
     "<hr>\n",
     "## Etapa 3: Entrenamiento, metaparámetros y regularización\n",
     "### Requerimiento general\n",
     "Diseñar y desarrollar un toolbox y documentación técnica del mismo, para entrenar una arquitectura de red neuronal, utilizando descenso del gradiente mediante backpropagation. Este toolbox deberá recibir una red (estructura de datos u objeto) y aplicar sobre la misma descenso del gradient para actualizar sus parámetros $W$ y $b$. Por otra parte, el toolbox deberá permitir ajustar los hiperparámetros de del entrenamiento (learning_rate, número de épocas y regularization_parameter), entre otras tareas.\n",
     "\n",
     "### Requerimientos específicos (mínimo)\n",
     "El toolbox creado debe cumplir con los siguientes requerimientos:\n",
     "- Debe permitir recibir una red neuronal no entrenada en forma de estructura de datos u objeto.\n",
     "- Debe permitir recibir un dataset compuesto por TRAIN_SET, DEV_SET y TEST_SET.\n",
     "- Debe permitir modificar los hiperparámetros del entrenamiento en forma de argumentos. Estos hiperparámetros son:\n",
     "    - Número de épocas de entrenamiento.\n",
     "    - Tasa de aprendizaje (learning rate).\n",
     "    - Parámetro de regularización.\n",
     "- Debe utilizar el toolbox de la etapa 2 para calcular la propagación hacia adelante, dentro de los ciclos de entrenamiento.\n",
     "- Debe permitir aplicar descenso del gradiente mediante backpropagation de forma automática, de acuerdo a la red que se ingresa como entrada.\n",
     "- Debe permitir acceder a los gradientes calculados $dZ$, $dW$ y $db$, durante el proceso de entrenamiento. \n",
     "- Debe implementar regularizacion L2 sobre la actualización de los parámetros de la red.\n",
     "- Debe permitir calcular e imprimir, el costo $J$ sobre el dataset de entrenamiento y el costo $J$ sobre el dataset de prueba, al final de las épocas de entrenamiento. El costo de entrenamiento se calcula mediante el TRAIN_SET y el costo de prueba se calcula mediante el DEV_SET."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "<hr>\n",
     "## Etapa 4: Experimentación, análisis y ajuste\n",
     "### Requerimiento general\n",
     "Esta etapa tiene como fin experimentar distintas arquitecturas de redes neuronales sobre el dataset DogsvsCats, y de manera iterativa, validar aspectos como el __bias__ y el __variance__ con el objetivo de encontrar una configuración de arquitectura y/o hiperparámetros que alcance un valor máximo (posiblemente 35% de error en el acierto o menor) sobre el dataset TEST_SET. Para ello, el grupo encargado, deberá utilizar los toolboxes y documentación de las etapas anteriores, para experimentar, analizar y ajustar una red neuronal para este propósito.\n",
     "\n",
     "### Requerimientos específicos (mínimo)\n",
     "Se deben cumplir los siguientes requerimientos:\n",
     "- Utilizar los toolboxes de las etapas 1, 2 y 3.\n",
     "- Construir una función para calcular el error en el acierto de una red neuronal sobre un dataset. Esta función debe tomar como entrada una red neuronal y un dataset y contar el número de aciertos o número de veces en que la clase $a$ que la red predice es igual a la clase real del vector $y$.\n",
     "### <center>$error = \\frac{m - conteo(a == y)}{m} * 100$</center>\n",
     "donde $m$ es el número de ejemplos en el dataset.\n",
     "<br><br>\n",
     "- Generar un conjunto de datasets utilizando el toolbox de la etapa 1, obteniendo así: TRAIN_SET, DEV_SET, TEST_SET.\n",
     "- Estos datasets deben tener inicialmente, un total de __1000__ ejemplos quedando así: TRAIN_SET (600 ejemplos), DEV_SET (200 ejemplos), TEST_SET (200 ejemplos).\n",
     "- Diseñar y construir una red neuronal inicial de dos capas ocultas y experimentar:\n",
     "    - Entrenar y probar utilizando TRAIN_SET y DEV_SET.\n",
     "    - Medir el bias (diferencia entre el error humano (supongamos un 10% de error en el acierto) y el error sobre TRAIN_SET) y variance (diferencia entre el error sobre el TRAIN_SET y el error sobre el DEV_SET).\n",
     "    - Visualizar algunos casos en que el sistema falla y discutir sobre posibles causas (utilizar el toolbox de la etapa 1).\n",
     "- Utilizando el valor del bias y variance, decidir sobre si:\n",
     "    - Aumentar el número de ejemplos > 1000.\n",
     "    - Cambiar la arquitectura de red: más capas y/o más neuronas por capas, diferente función de activación en las capas ocultas.\n",
     "    - Aumentar o disminuir el valor de regularización.\n",
     "    - Aumentar o disminuir el número de épocas de entrenamiento.\n",
     "- REPETIR los pasos anteriores y DOCUMENTAR, hasta encontrar un valor de error en el acierto cercano o menor que el propuesto.\n",
     "- Una vez se encuentre el menor valor posible de error en el acierto, probar la red entrenada sobre el dataset TEST_SET y medir el error. Si este error es cercano o menor que el 35% terminar o seguir experimentando."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
     "collapsed": true
    },
    "outputs": [],
    "source": []
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 2",
    "language": "python",
    "name": "python2"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 2
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython2",
    "version": "2.7.13"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 2
 }
